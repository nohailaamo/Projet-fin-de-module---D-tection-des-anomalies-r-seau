import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.cluster import DBSCAN, KMeans
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, davies_bouldin_score
from kneed import KneeLocator
import time

# Configuration de la page
st.set_page_config(page_title="D√©tection d'anomalies R√©seau",page_icon="applogo.png", layout="wide")
st.title("D√©tection d'anomalies R√©seau")

# Sidebar pour les configurations
st.sidebar.title("‚öôÔ∏è Configuration")

# Section upload
uploaded_file = st.sidebar.file_uploader("Choisissez un fichier CSV pour analyse", type=['csv'])

if uploaded_file is not None:
    # Chargement des donn√©es avec feedback utilisateur
    with st.spinner('Chargement des donn√©es...'):
        try:
            # Essayer d'abord avec header
            df = pd.read_csv(uploaded_file)
            # V√©rifier si les noms de colonnes semblent √™tre des donn√©es
            if df.columns.str.contains(r'^\d+$').all():
                df = pd.read_csv(uploaded_file, header=None)
        except:
            # Si √©chec, essayer sans header
            df = pd.read_csv(uploaded_file, header=None)
        
        # Cr√©ation d'une copie des donn√©es originales
        df_original = df.copy()
    
    # Affichage des informations sur le dataset
    st.subheader("üìã Informations sur le dataset")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Nombre d'observations", df.shape[0])
        st.metric("Nombre de caract√©ristiques", df.shape[1])
    with col2:
        missing_values = df.isnull().sum().sum()
        st.metric("Valeurs manquantes", missing_values)
        if missing_values > 0:
            st.warning(f"‚ö†Ô∏è Le dataset contient {missing_values} valeurs manquantes.")
    
    # Aper√ßu des donn√©es
    with st.expander("üîé Aper√ßu des donn√©es brutes"):
        st.write(df.head(10))
        
        # Informations sur les types de donn√©es
        st.subheader("Types de donn√©es")
        dtypes_df = pd.DataFrame(df.dtypes, columns=['Type de donn√©es'])
        st.write(dtypes_df)

    # Pr√©traitement des donn√©es
    st.subheader("üîß Pr√©traitement des donn√©es")
    
    # S√©lection des colonnes
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # Interface pour s√©lectionner les colonnes √† utiliser
    with st.expander("S√©lection des caract√©ristiques"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("Caract√©ristiques num√©riques:")
            selected_numeric = st.multiselect(
                "S√©lectionnez les caract√©ristiques num√©riques √† utiliser",
                numeric_cols,
                default=numeric_cols
            )
        
        with col2:
            st.write("Caract√©ristiques cat√©gorielles:")
            selected_categorical = st.multiselect(
                "S√©lectionnez les caract√©ristiques cat√©gorielles √† utiliser",
                categorical_cols,
                default=categorical_cols
            )
    
    # Traitement des valeurs manquantes
    handle_missing = st.radio(
        "Traitement des valeurs manquantes:",
        ("Supprimer les lignes avec valeurs manquantes", "Remplacer par la moyenne/mode")
    )
    
    with st.spinner('Pr√©traitement en cours...'):
        # Application du traitement des valeurs manquantes
        if handle_missing == "Supprimer les lignes avec valeurs manquantes":
            df_clean = df[selected_numeric + selected_categorical].dropna()
        else:
            df_clean = df[selected_numeric + selected_categorical].copy()
            # Remplacement des valeurs manquantes num√©riques par la moyenne
            for col in selected_numeric:
                df_clean[col] = df_clean[col].fillna(df_clean[col].mean())
            # Remplacement des valeurs manquantes cat√©gorielles par le mode
            for col in selected_categorical:
                if df_clean[col].dtype == 'object' or df_clean[col].dtype.name == 'category':
                    df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0])
        
        # V√©rification apr√®s pr√©traitement
        if df_clean.empty:
            st.error("‚ùå Apr√®s pr√©traitement, le dataset est vide. Veuillez revoir vos param√®tres.")
            st.stop()
        
        # Pr√©paration des donn√©es pour le clustering
        if len(selected_numeric) > 0 or len(selected_categorical) > 0:
            # Cr√©ation d'un pr√©processeur
            preprocessor_steps = []
            
            if selected_numeric:
                preprocessor_steps.append(('num', StandardScaler(), selected_numeric))
            
            if selected_categorical:
                preprocessor_steps.append(('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), selected_categorical))
            
            if preprocessor_steps:
                preprocessor = ColumnTransformer(preprocessor_steps)
                X_preprocessed = preprocessor.fit_transform(df_clean)
                
                # Cr√©ation d'un DataFrame des donn√©es pr√©trait√©es pour l'affichage
                if len(selected_categorical) > 0:
                    # R√©cup√©ration des noms des colonnes apr√®s one-hot encoding
                    cat_features = []
                    for i, category in enumerate(selected_categorical):
                        cat_encoder = preprocessor.transformers_[1][1]
                        categories = cat_encoder.categories_[i]
                        for cat in categories:
                            cat_features.append(f"{category}_{cat}")
                    
                    feature_names = selected_numeric + cat_features
                else:
                    feature_names = selected_numeric
                
                # Assurer que nous avons le bon nombre de noms de colonnes
                if len(feature_names) != X_preprocessed.shape[1]:
                    feature_names = [f"feature_{i}" for i in range(X_preprocessed.shape[1])]
                
                df_preprocessed = pd.DataFrame(X_preprocessed, columns=feature_names)
            else:
                st.error("‚ùå Veuillez s√©lectionner au moins une caract√©ristique.")
                st.stop()
        else:
            st.error("‚ùå Aucune caract√©ristique s√©lectionn√©e. Veuillez en choisir au moins une.")
            st.stop()
    
    # Affichage des statistiques descriptives
    with st.expander("üìä Statistiques descriptives"):
        if len(selected_numeric) > 0:
            st.write(df_clean[selected_numeric].describe())
        
        # Matrice de corr√©lation pour les variables num√©riques
        if len(selected_numeric) > 1:
            st.subheader("Matrice de corr√©lation")
            corr_matrix = df_clean[selected_numeric].corr()
            fig, ax = plt.subplots(figsize=(10, 8))
            cmap = sns.diverging_palette(230, 20, as_cmap=True)
            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
            sns.heatmap(corr_matrix, mask=mask, cmap=cmap, center=0,
                        square=True, linewidths=.5, annot=True, fmt=".2f")
            st.pyplot(fig)
    
    # Analyse en Composantes Principales
    with st.expander("üß© Analyse en Composantes Principales (PCA)"):
        n_components = min(5, X_preprocessed.shape[1])
        n_components = st.slider("Nombre de composantes principales", 2, n_components, 2)
        
        pca = PCA(n_components=n_components)
        pca_result = pca.fit_transform(X_preprocessed)
        
        # Variance expliqu√©e
        explained_variance = pca.explained_variance_ratio_
        cumulative_variance = np.cumsum(explained_variance)
        
        # Graphique de la variance expliqu√©e
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='skyblue',
                label='Variance individuelle')
        ax.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', 
                label='Variance cumul√©e')
        ax.set_xlabel('Composantes principales')
        ax.set_ylabel('Ratio de variance expliqu√©e')
        ax.set_title('Variance expliqu√©e par composante principale')
        ax.legend()
        ax.grid(True, linestyle='--', alpha=0.7)
        st.pyplot(fig)
        
        # Affichage des 2 premi√®res composantes
        st.subheader("Visualisation des 2 premi√®res composantes principales")
        pca_df = pd.DataFrame(data=pca_result[:, :2], columns=['PC1', 'PC2'])
        
        fig = px.scatter(pca_df, x='PC1', y='PC2', opacity=0.7,
                        labels={'PC1': f'PC1 ({explained_variance[0]:.2%})',
                                'PC2': f'PC2 ({explained_variance[1]:.2%})'},
                        title='Projection PCA des donn√©es')
        st.plotly_chart(fig, use_container_width=True)
    
    # Algorithmes de clustering
    st.subheader("üß† Algorithmes de clustering")
    
    # S√©lection de l'algorithme
    algo_choice = st.radio(
        "Choisissez les algorithmes √† ex√©cuter:",
        ("K-Means", "DBSCAN", "Isolation Forest", "Tous")
    )
    
    # K-Means
    if algo_choice in ["K-Means", "Tous"]:
        st.markdown("### K-Means")
        
        # D√©termination du nombre optimal de clusters (m√©thode du coude)
        with st.expander("üîç Trouver le nombre optimal de clusters"):
            max_clusters = min(15, X_preprocessed.shape[0] // 5)  # Limite raisonnable
            
            if st.button("Calculer la m√©thode du coude"):
                with st.spinner("Calcul en cours..."):
                    inertia = []
                    K_range = range(1, max_clusters + 1)
                    
                    for k in K_range:
                        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
                        kmeans.fit(X_preprocessed)
                        inertia.append(kmeans.inertia_)
                    
                    # M√©thode du coude
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.plot(K_range, inertia, 'bo-')
                    ax.set_xlabel('Nombre de clusters')
                    ax.set_ylabel('Inertie')
                    ax.set_title('M√©thode du coude pour d√©terminer le nombre optimal de clusters')
                    ax.grid(True, linestyle='--', alpha=0.7)
                    
                    # D√©tection automatique du coude
                    try:
                        kl = KneeLocator(K_range, inertia, curve='convex', direction='decreasing')
                        optimal_k = kl.elbow
                        ax.axvline(x=optimal_k, color='r', linestyle='--', 
                                  label=f'Coude d√©tect√© √† k={optimal_k}')
                        ax.legend()
                    except:
                        st.warning("‚ö†Ô∏è Impossible de d√©tecter automatiquement le coude.")
                    
                    st.pyplot(fig)
        
        # Param√®tres K-Means
        n_clusters = st.slider("Nombre de clusters pour K-Means", 2, 10, 3)
        
        # Ex√©cution de K-Means
        with st.spinner("Ex√©cution de K-Means..."):
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
            kmeans_labels = kmeans.fit_predict(X_preprocessed)
            
            # Ajout des labels au DataFrame
            df_clean['KMeans_Cluster'] = kmeans_labels
            
            # M√©triques d'√©valuation
            if n_clusters > 1 and n_clusters < len(X_preprocessed):
                silhouette_avg = silhouette_score(X_preprocessed, kmeans_labels)
                davies_bouldin = davies_bouldin_score(X_preprocessed, kmeans_labels)
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Score Silhouette", f"{silhouette_avg:.3f}",
                             delta="+0.1" if silhouette_avg > 0.5 else "-0.1", 
                             delta_color="normal")
                with col2:
                    st.metric("Score Davies-Bouldin", f"{davies_bouldin:.3f}",
                             delta="-0.1" if davies_bouldin < 1.0 else "+0.1", 
                             delta_color="inverse")
            
            # Visualisation PCA avec clusters
            pca_vis = PCA(n_components=2)
            pca_result = pca_vis.fit_transform(X_preprocessed)
            
            pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])
            pca_df['Cluster'] = kmeans_labels
            
            fig = px.scatter(pca_df, x='PC1', y='PC2', color='Cluster', 
                            color_continuous_scale=px.colors.qualitative.G10,
                            labels={'PC1': 'Composante Principale 1', 
                                    'PC2': 'Composante Principale 2'},
                            title='Visualisation des clusters K-Means (PCA)')
            st.plotly_chart(fig, use_container_width=True)
            
            # Analyse des clusters
            st.subheader("Analyse des clusters K-Means")
            
            # Distribution des observations par cluster
            cluster_counts = pd.DataFrame(df_clean['KMeans_Cluster'].value_counts()).reset_index()
            cluster_counts.columns = ['Cluster', 'Nombre d\'observations']
            
            fig = px.bar(cluster_counts, x='Cluster', y='Nombre d\'observations',
                        title='Distribution des observations par cluster')
            st.plotly_chart(fig, use_container_width=True)
            
            # Statistiques des clusters
            with st.expander("üìä Statistiques d√©taill√©es par cluster"):
                selected_cluster = st.selectbox(
                    "S√©lectionnez un cluster pour voir ses statistiques",
                    sorted(df_clean['KMeans_Cluster'].unique())
                )
                
                if selected_numeric:
                    st.write(df_clean[df_clean['KMeans_Cluster'] == selected_cluster][selected_numeric].describe())
                    
                    # Boxplots des caract√©ristiques num√©riques
                    if len(selected_numeric) > 0:
                        fig = px.box(df_clean[df_clean['KMeans_Cluster'] == selected_cluster],
                                    y=selected_numeric, 
                                    title=f'Distribution des caract√©ristiques pour le cluster {selected_cluster}')
                        st.plotly_chart(fig, use_container_width=True)
                
                # Caract√©ristiques cat√©gorielles
                if selected_categorical:
                    for cat in selected_categorical:
                        if df_clean[cat].dtype == 'object' or df_clean[cat].dtype.name == 'category':
                            cat_dist = df_clean[df_clean['KMeans_Cluster'] == selected_cluster][cat].value_counts()
                            
                            fig = px.pie(values=cat_dist.values, names=cat_dist.index,
                                        title=f'Distribution de {cat} dans le cluster {selected_cluster}')
                            st.plotly_chart(fig, use_container_width=True)
            
            # Comparaison des centro√Ødes
            with st.expander("üìç Centro√Ødes des clusters"):
                # R√©cup√©ration des centro√Ødes
                centroids = pd.DataFrame(kmeans.cluster_centers_, columns=df_preprocessed.columns)
                
                # Heatmap des centro√Ødes
                fig = px.imshow(centroids, text_auto=True,
                                labels=dict(x="Caract√©ristique", y="Cluster", color="Valeur"),
                                title="Heatmap des centro√Ødes des clusters")
                st.plotly_chart(fig, use_container_width=True)
                
                # Tableau des centro√Ødes
                st.write(centroids)
    
    # DBSCAN
    if algo_choice in ["DBSCAN", "Tous"]:
        st.markdown("### DBSCAN")
        
        # Param√®tres DBSCAN
        col1, col2 = st.columns(2)
        with col1:
            eps_value = st.slider("Epsilon (eps)", 0.1, 2.0, 0.5, 0.1)
        with col2:
            min_samples = st.slider("Minimum d'√©chantillons", 2, 20, 5)
        
        # Ex√©cution de DBSCAN
        with st.spinner("Ex√©cution de DBSCAN..."):
            dbscan = DBSCAN(eps=eps_value, min_samples=min_samples)
            dbscan_labels = dbscan.fit_predict(X_preprocessed)
            
            # Ajout des labels au DataFrame
            df_clean['DBSCAN_Cluster'] = dbscan_labels
            
            # M√©triques
            n_clusters_ = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
            n_noise_ = list(dbscan_labels).count(-1)
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Nombre de clusters identifi√©s", n_clusters_)
            with col2:
                noise_percent = n_noise_ / len(dbscan_labels) * 100
                st.metric("Points de bruit", f"{n_noise_} ({noise_percent:.1f}%)")
            
            # Visualisation PCA avec clusters DBSCAN
            pca_vis = PCA(n_components=2)
            pca_result = pca_vis.fit_transform(X_preprocessed)
            
            pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])
            pca_df['Cluster'] = ["Bruit" if l == -1 else f"Cluster {l}" for l in dbscan_labels]
            
            fig = px.scatter(pca_df, x='PC1', y='PC2', color='Cluster',
                            labels={'PC1': 'Composante Principale 1', 
                                    'PC2': 'Composante Principale 2'},
                            title='Visualisation des clusters DBSCAN (PCA)')
            st.plotly_chart(fig, use_container_width=True)
            
            # Analyse des clusters DBSCAN
            st.subheader("Analyse des clusters DBSCAN")
            
            # Distribution des observations par cluster
            cluster_counts = pd.DataFrame(df_clean['DBSCAN_Cluster'].value_counts()).reset_index()
            cluster_counts.columns = ['Cluster', 'Nombre d\'observations']
            
            fig = px.bar(cluster_counts, x='Cluster', y='Nombre d\'observations',
                        title='Distribution des observations par cluster')
            st.plotly_chart(fig, use_container_width=True)
            
            # Comparaison de t-SNE
            with st.expander("üîÑ Visualisation t-SNE"):
                st.info("t-SNE est particuli√®rement utile pour visualiser des donn√©es √† haute dimension")
                
                with st.spinner("Calcul de t-SNE en cours..."):
                    # Application de t-SNE
                    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_preprocessed)-1))
                    tsne_results = tsne.fit_transform(X_preprocessed)
                    
                    # Cr√©ation du DataFrame pour la visualisation
                    tsne_df = pd.DataFrame(data=tsne_results, columns=['t-SNE1', 't-SNE2'])
                    tsne_df['Cluster'] = ["Bruit" if l == -1 else f"Cluster {l}" for l in dbscan_labels]
                    
                    # Cr√©ation du graphique
                    fig = px.scatter(tsne_df, x='t-SNE1', y='t-SNE2', color='Cluster',
                                    title='Visualisation t-SNE des clusters DBSCAN')
                    st.plotly_chart(fig, use_container_width=True)
    
    # Isolation Forest
    if algo_choice in ["Isolation Forest", "Tous"]:
        st.markdown("### Isolation Forest")
        
        # Param√®tres Isolation Forest
        contamination = st.slider("Taux de contamination attendu", 0.01, 0.5, 0.1, 0.01)
        
        # Ex√©cution de Isolation Forest
        with st.spinner("Ex√©cution de Isolation Forest..."):
            iso_forest = IsolationForest(contamination=contamination, random_state=42)
            iso_forest_pred = iso_forest.fit_predict(X_preprocessed)
            
            # Conversion des pr√©dictions (-1 pour anomalie, 1 pour normal) en labels binaires
            iso_forest_labels = np.where(iso_forest_pred == -1, 1, 0)  # 1 pour anomalie, 0 pour normal
            
            # Ajout des labels au DataFrame
            df_clean['IsAnomaly'] = iso_forest_labels
            
            # Calcul des scores d'anomalie
            anomaly_scores = -iso_forest.score_samples(X_preprocessed)
            df_clean['AnomalyScore'] = anomaly_scores
            
            # Statistiques sur les anomalies
            n_anomalies = np.sum(iso_forest_labels)
            anomaly_percent = n_anomalies / len(iso_forest_labels) * 100
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Nombre d'anomalies d√©tect√©es", n_anomalies)
            with col2:
                st.metric("Pourcentage d'anomalies", f"{anomaly_percent:.1f}%")
            
            # Visualisation des anomalies
            pca_vis = PCA(n_components=2)
            pca_result = pca_vis.fit_transform(X_preprocessed)
            
            pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])
            pca_df['Anomalie'] = ["Anomalie" if l == 1 else "Normal" for l in iso_forest_labels]
            pca_df['Score d\'anomalie'] = anomaly_scores
            
            fig = px.scatter(pca_df, x='PC1', y='PC2', color='Anomalie',
                            color_discrete_map={'Normal': 'blue', 'Anomalie': 'red'},
                            labels={'PC1': 'Composante Principale 1', 
                                    'PC2': 'Composante Principale 2'},
                            title='Visualisation des anomalies (Isolation Forest)')
            st.plotly_chart(fig, use_container_width=True)
            
            # Distribution des scores d'anomalie
            fig = px.histogram(pca_df, x='Score d\'anomalie', color='Anomalie',
                              title='Distribution des scores d\'anomalie',
                              color_discrete_map={'Normal': 'blue', 'Anomalie': 'red'})
            st.plotly_chart(fig, use_container_width=True)
            
            # Top des anomalies
            st.subheader("Top 10 des anomalies les plus significatives")
            top_anomalies = df_clean[df_clean['IsAnomaly'] == 1].sort_values(by='AnomalyScore', ascending=False).head(10)
            st.write(top_anomalies)
    
    # Comparaison des algorithmes
    if algo_choice == "Tous":
        st.subheader("üìä Comparaison des algorithmes")
        
        # Cr√©ation d'un DataFrame de r√©sultats
        results_df = df_clean.copy()
        
        # Tableau de contingence K-Means vs DBSCAN
        st.write("K-Means vs DBSCAN")
        contingency = pd.crosstab(results_df['KMeans_Cluster'], results_df['DBSCAN_Cluster'])
        st.write(contingency)
        
        # Visualisation de la comparaison
        fig = px.scatter(pca_df, x='PC1', y='PC2', color='Cluster',
                        symbol='Anomalie', symbol_map={'Normal': 'circle', 'Anomalie': 'x'},
                        title='Comparaison des r√©sultats des diff√©rents algorithmes')
        st.plotly_chart(fig, use_container_width=True)
    
    # Export des r√©sultats
    st.subheader("üì• Exportation des r√©sultats")
    
    # Pr√©paration des donn√©es pour l'export
    export_df = df_original.copy()
    
    if 'KMeans_Cluster' in df_clean.columns:
        # Aligner les indices
        export_df = export_df.loc[df_clean.index]
        export_df['KMeans_Cluster'] = df_clean['KMeans_Cluster']
    
    if 'DBSCAN_Cluster' in df_clean.columns:
        if 'KMeans_Cluster' not in df_clean.columns:
            # Aligner les indices si pas d√©j√† fait
            export_df = export_df.loc[df_clean.index]
        export_df['DBSCAN_Cluster'] = df_clean['DBSCAN_Cluster']
    
    if 'IsAnomaly' in df_clean.columns:
        if 'KMeans_Cluster' not in df_clean.columns and 'DBSCAN_Cluster' not in df_clean.columns:
            # Aligner les indices si pas d√©j√† fait
            export_df = export_df.loc[df_clean.index]
        export_df['IsAnomaly'] = df_clean['IsAnomaly']
        export_df['AnomalyScore'] = df_clean['AnomalyScore']
    
    # Export en CSV
    csv = export_df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="üì• T√©l√©charger les r√©sultats (CSV)",
        data=csv,
        file_name="resultats_detection_anomalies.csv",
        mime="text/csv",
    )
    
    # Dashboard synth√©tique
    st.subheader("üìä Dashboard de synth√®se")
    
    # Cr√©ation d'une vue synth√©tique
    with st.container():
        st.markdown("### Vue d'ensemble des r√©sultats")
        
        # Cr√©ation d'une disposition en grille
        col1, col2 = st.columns(2)
        
        with col1:
            # R√©sum√© des algorithmes et leurs r√©sultats
            summary_data = []
            
            if 'KMeans_Cluster' in df_clean.columns:
                n_kmeans_clusters = len(df_clean['KMeans_Cluster'].unique())
                summary_data.append({
                    "Algorithme": "K-Means",
                    "Clusters d√©tect√©s": n_kmeans_clusters,
                    "Points classifi√©s": df_clean.shape[0]
                })
            
            if 'DBSCAN_Cluster' in df_clean.columns:
                n_dbscan_clusters = len(set(df_clean['DBSCAN_Cluster'])) - (1 if -1 in df_clean['DBSCAN_Cluster'].values else 0)
                n_noise = list(df_clean['DBSCAN_Cluster']).count(-1)
                summary_data.append({
                    "Algorithme": "DBSCAN",
                    "Clusters d√©tect√©s": n_dbscan_clusters,
                    "Points classifi√©s": df_clean.shape[0] - n_noise,
                    "Points de bruit": n_noise
                })
            
            if 'IsAnomaly' in df_clean.columns:
                n_anomalies = df_clean['IsAnomaly'].sum()
                summary_data.append({
                    "Algorithme": "Isolation Forest",
                    "Anomalies d√©tect√©es": n_anomalies,
                    "Points normaux": df_clean.shape[0] - n_anomalies,
                    "Taux d'anomalies": f"{(n_anomalies / df_clean.shape[0] * 100):.2f}%"
                })
            
            if summary_data:
                summary_df = pd.DataFrame(summary_data)
                st.write(summary_df)
        
        with col2:
            # Graphique r√©capitulatif des r√©sultats
            if 'IsAnomaly' in df_clean.columns:
                anomaly_counts = df_clean['IsAnomaly'].value_counts().reset_index()
                anomaly_counts.columns = ['Type', 'Count']
                anomaly_counts['Type'] = anomaly_counts['Type'].map({0: 'Normal', 1: 'Anomalie'})
                
                fig = px.pie(anomaly_counts, values='Count', names='Type', 
                            title='R√©partition des anomalies',
                            color='Type', color_discrete_map={'Normal': 'blue', 'Anomalie': 'red'})
                st.plotly_chart(fig, use_container_width=True)
            elif 'DBSCAN_Cluster' in df_clean.columns:
                dbscan_counts = df_clean['DBSCAN_Cluster'].value_counts().reset_index()
                dbscan_counts.columns = ['Cluster', 'Count']
                
                fig = px.pie(dbscan_counts, values='Count', names='Cluster', 
                            title='R√©partition des clusters DBSCAN')
                st.plotly_chart(fig, use_container_width=True)
    
    # Section d'interpr√©tation des r√©sultats
    st.markdown("### üîç Interpr√©tation des r√©sultats")
    
    with st.expander("Guide d'interpr√©tation"):
        st.markdown("""
        #### Comment interpr√©ter les r√©sultats de la d√©tection d'anomalies
        
        1. **K-Means**:
           - Les clusters repr√©sentent des groupes de points avec des caract√©ristiques similaires
           - Les points isol√©s ou dans de petits clusters peuvent indiquer des comportements atypiques
           - Examinez les caract√©ristiques des centro√Ødes pour comprendre ce qui d√©finit chaque cluster
        
        2. **DBSCAN**:
           - Les points de bruit (label -1) sont consid√©r√©s comme des anomalies
           - L'algorithme identifie des clusters bas√©s sur la densit√© des points
           - Particuli√®rement utile pour d√©tecter des attaques distribu√©es ou des comportements anormaux dans le r√©seau
        
        3. **Isolation Forest**:
           - Les points identifi√©s comme anomalies ont un score d'anomalie √©lev√©
           - Efficace pour d√©tecter des observations qui se distinguent facilement du reste des donn√©es
           - Examinez les caract√©ristiques des points avec les scores d'anomalie les plus √©lev√©s
        
        #### Types d'anomalies r√©seau courantes:
        
        1. **Scan de ports**: Tentatives d'identification des services actifs sur un r√©seau
        2. **Attaques par d√©ni de service (DoS/DDoS)**: Tentatives de surcharge du r√©seau
        3. **Exfiltration de donn√©es**: Transferts anormaux de grandes quantit√©s de donn√©es
        4. **Mouvements lat√©raux**: Tentatives de se d√©placer √† travers le r√©seau apr√®s une compromission
        5. **Comportements d'utilisateurs anormaux**: Connexions √† des heures inhabituelles ou √† partir d'emplacements non reconnus
        """)
    
    # Recommandations de s√©curit√©
    st.markdown("### üõ°Ô∏è Recommandations de s√©curit√©")
    
    with st.expander("Actions recommand√©es"):
        st.markdown("""
        #### Actions recommand√©es suite √† la d√©tection d'anomalies
        
        1. **Investigation approfondie**:
           - Analysez les paquets r√©seau li√©s aux anomalies d√©tect√©es
           - V√©rifiez les journaux syst√®me et applicatifs pour les activit√©s suspectes
           - Corr√©lation avec d'autres √©v√©nements de s√©curit√©
        
        2. **Isolation et rem√©diation**:
           - Isolez temporairement les syst√®mes suspect√©s d'√™tre compromis
           - Appliquez les correctifs de s√©curit√© manquants
           - Renforcez les configurations de s√©curit√©
        
        3. **Am√©lioration du mod√®le**:
           - Utilisez les feedbacks des analyses pour am√©liorer le mod√®le
           - Ajustez les param√®tres des algorithmes en fonction des r√©sultats
           - Envisagez d'incorporer des techniques d'apprentissage supervis√© si des labels sont disponibles
        
        4. **Mise en place de contre-mesures**:
           - Configurez des r√®gles de d√©tection dans les outils de s√©curit√© existants
           - Impl√©mentez des politiques de s√©curit√© plus strictes pour les comportements identifi√©s comme risqu√©s
           - Envisagez des solutions automatis√©es de r√©ponse aux incidents
        """)
    
    # Int√©gration et d√©ploiement en production
    st.markdown("### üöÄ Int√©gration et d√©ploiement")
    
    with st.expander("Options de d√©ploiement"):
        deployment_option = st.selectbox(
            "S√©lectionnez une option de d√©ploiement:",
            ["Application Streamlit", "API avec FastAPI", "Service Cloud (AWS/GCP/Azure)", "Int√©gration SIEM"]
        )
        
        if deployment_option == "Application Streamlit":
            st.markdown("""
            #### D√©ploiement Streamlit
            
            ```bash
            # Installation des d√©pendances
            pip install -r requirements.txt
            
            # Lancement de l'application
            streamlit run app.py
            ```
            
            Pour d√©ployer sur Streamlit Cloud:
            1. Cr√©ez un d√©p√¥t GitHub avec votre code
            2. Connectez-vous √† [Streamlit Cloud](https://streamlit.io/cloud)
            3. D√©ployez l'application depuis votre d√©p√¥t GitHub
            """)
        
        elif deployment_option == "API avec FastAPI":
            st.markdown("""
            #### D√©ploiement avec FastAPI
            
            ```python
            # app.py
            from fastapi import FastAPI, UploadFile, File
            import pandas as pd
            from sklearn.preprocessing import StandardScaler
            from sklearn.cluster import DBSCAN
            import joblib
            
            app = FastAPI()
            
            # Chargez votre mod√®le pr√©-entra√Æn√©
            model = joblib.load('model/dbscan_model.pkl')
            scaler = joblib.load('model/scaler.pkl')
            
            @app.post("/predict/")
            async def predict(file: UploadFile = File(...)):
                # Traitement du fichier
                df = pd.read_csv(file.file)
                
                # Pr√©traitement
                df_numeric = df.select_dtypes(include=['number'])
                X_scaled = scaler.transform(df_numeric)
                
                # Pr√©diction
                predictions = model.fit_predict(X_scaled)
                
                # Formatage des r√©sultats
                results = {
                    "anomalies": int((predictions == -1).sum()),
                    "normal": int((predictions != -1).sum()),
                    "clusters": int(len(set(predictions)) - (1 if -1 in predictions else 0)),
                    "predictions": predictions.tolist()
                }
                
                return results
            
            # Lancement: uvicorn app:app --reload
            ```
            """)
        
        elif deployment_option == "Service Cloud (AWS/GCP/Azure)":
            st.markdown("""
            #### D√©ploiement sur AWS
            
            1. **AWS Lambda avec API Gateway**:
               - Packagez votre mod√®le et code
               - Cr√©ez une fonction Lambda
               - Configurez API Gateway comme point d'entr√©e
            
            2. **AWS SageMaker**:
               - Utiliser SageMaker pour entra√Æner et d√©ployer le mod√®le
               - Cr√©er un endpoint accessible via API
            
            3. **AWS ECS/EKS**:
               - Conteneurisez votre application avec Docker
               - D√©ployez sur ECS ou EKS pour une solution scalable
            
            #### Exemple de Dockerfile
            ```dockerfile
            FROM python:3.9-slim
            
            WORKDIR /app
            
            COPY requirements.txt .
            RUN pip install --no-cache-dir -r requirements.txt
            
            COPY . .
            
            EXPOSE 8000
            
            CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
            ```
            """)
        
        elif deployment_option == "Int√©gration SIEM":
            st.markdown("""
            #### Int√©gration avec des solutions SIEM
            
            1. **Splunk**:
               - Cr√©ez une application Splunk personnalis√©e
               - Utilisez le SDK Splunk pour Python pour envoyer les alertes
               - Configurez des dashboards pour visualiser les anomalies
            
            2. **Elastic Stack (ELK)**:
               - Envoyez les r√©sultats √† Elasticsearch
               - Cr√©ez des visualisations Kibana
               - Configurez des alertes bas√©es sur les d√©tections
            
            3. **QRadar/ArcSight**:
               - Utilisez les API REST pour envoyer des √©v√©nements
               - Configurez des r√®gles de corr√©lation bas√©es sur les r√©sultats du mod√®le
            
            ```python
            # Exemple d'int√©gration avec Elasticsearch
            from elasticsearch import Elasticsearch
            
            es = Elasticsearch(['http://localhost:9200'])
            
            def send_anomaly_to_elk(anomaly_data):
                doc = {
                    'timestamp': datetime.now(),
                    'source_ip': anomaly_data['source_ip'],
                    'destination_ip': anomaly_data['destination_ip'],
                    'score': anomaly_data['anomaly_score'],
                    'algorithm': 'isolation_forest',
                    'features': anomaly_data['features']
                }
                
                res = es.index(index="network-anomalies", document=doc)
                return res
            ```
            """)
    
    # Futur et am√©liorations
    st.markdown("### üîÆ Am√©liorations futures")
    
    future_improvements = st.multiselect(
        "S√©lectionnez les axes d'am√©lioration int√©ressants pour votre projet:",
        [
            "Int√©gration de donn√©es temps r√©el",
            "Apprentissage incr√©mental/adaptatif",
            "Ajout d'algorithmes suppl√©mentaires",
            "Optimisation des hyperparam√®tres automatique",
            "Visualisations avanc√©es",
            "Analyse explicative des anomalies",
            "D√©ploiement en production",
            "Interface utilisateur plus intuitive"
        ],
        default=["Apprentissage incr√©mental/adaptatif", "Analyse explicative des anomalies"]
    )
    
    if future_improvements:
        st.write("Axes d'am√©lioration s√©lectionn√©s:")
        
        if "Int√©gration de donn√©es temps r√©el" in future_improvements:
            st.markdown("""
            - **Int√©gration de donn√©es temps r√©el**: Utiliser Kafka ou RabbitMQ pour traiter des flux continus de donn√©es r√©seau et d√©tecter les anomalies en temps r√©el.
            """)
        
        if "Apprentissage incr√©mental/adaptatif" in future_improvements:
            st.markdown("""
            - **Apprentissage incr√©mental/adaptatif**: Impl√©menter des mod√®les qui s'adaptent au fil du temps aux changements dans les patterns r√©seau l√©gitimes, r√©duisant ainsi les faux positifs.
            """)
        
        if "Ajout d'algorithmes suppl√©mentaires" in future_improvements:
            st.markdown("""
            - **Ajout d'algorithmes suppl√©mentaires**: Explorer d'autres algorithmes comme One-Class SVM, Local Outlier Factor, ou des approches bas√©es sur des autoencodeurs.
            """)
        
        if "Optimisation des hyperparam√®tres automatique" in future_improvements:
            st.markdown("""
            - **Optimisation des hyperparam√®tres automatique**: Utiliser des techniques comme Grid Search, Random Search ou Bayesian Optimization pour trouver les meilleurs param√®tres.
            """)
        
        if "Visualisations avanc√©es" in future_improvements:
            st.markdown("""
            - **Visualisations avanc√©es**: Impl√©menter des graphes de r√©seau interactifs, des cartes de chaleur temporelles et d'autres visualisations sp√©cifiques au domaine de la cybers√©curit√©.
            """)
        
        if "Analyse explicative des anomalies" in future_improvements:
            st.markdown("""
            - **Analyse explicative des anomalies**: D√©velopper des techniques pour expliquer pourquoi certains points sont consid√©r√©s comme des anomalies, facilitant le travail des analystes de s√©curit√©.
            """)
        
        if "D√©ploiement en production" in future_improvements:
            st.markdown("""
            - **D√©ploiement en production**: Mettre en place un pipeline CI/CD, g√©rer les versions du mod√®le, et impl√©menter une surveillance des performances du mod√®le en production.
            """)
        
        if "Interface utilisateur plus intuitive" in future_improvements:
            st.markdown("""
            - **Interface utilisateur plus intuitive**: Am√©liorer l'UX/UI avec des tableaux de bord personnalisables, des indicateurs visuels simplifi√©s pour les non-experts et des workflows guid√©s.
            """)

else:
    # Page d'accueil lorsqu'aucun fichier n'est charg√©
    st.markdown("""
    # üåê D√©tection d'Anomalies R√©seau par Apprentissage Non Supervis√©
    
    ## üéØ Objectif du projet
    
    Cette application vous permet de d√©tecter des comportements anormaux dans votre trafic r√©seau en utilisant des algorithmes d'apprentissage non supervis√©, notamment:
    
    - **K-Means**: Regroupement bas√© sur la similarit√© des caract√©ristiques
    - **DBSCAN**: Identification de clusters bas√©s sur la densit√©
    - **Isolation Forest**: D√©tection d'anomalies bas√©e sur l'isolation
    
    ## üìä Fonctionnalit√©s
    
    - Pr√©traitement automatique des donn√©es r√©seau
    - Visualisation des clusters et anomalies
    - Analyse comparative des algorithmes
    - Exportation des r√©sultats
    - Recommandations de s√©curit√© bas√©es sur les anomalies d√©tect√©es
    
    ## üöÄ Pour commencer
    
    1. Chargez votre fichier CSV contenant les donn√©es r√©seau
    2. Configurez les param√®tres des algorithmes
    3. Analysez les r√©sultats et visualisations
    4. Exportez les r√©sultats pour une analyse plus approfondie
    
    ## üìã Format de donn√©es recommand√©
    
    Le format id√©al est un fichier CSV contenant des caract√©ristiques de trafic r√©seau comme:
    - Adresses IP source/destination
    - Ports source/destination
    - Protocole
    - Nombre de paquets/octets
    - Dur√©e des connexions
    - Flags de connexion TCP
    
    Des jeux de donn√©es publics comme NSL-KDD, CICIDS2017 ou UNSW-NB15 sont √©galement compatibles.
    
    ## üîí Confidentialit√©
    
    Toutes les donn√©es sont trait√©es localement et ne sont pas partag√©es √† des tiers.
    """)
    
    # Exemple de dataset
    st.markdown("### ‚¨áÔ∏è T√©l√©charger un exemple de dataset")
    
    sample_data = """
    src_ip,dst_ip,src_port,dst_port,protocol,packets,bytes,duration,flags
    192.168.1.1,10.0.0.1,53210,80,TCP,12,1020,0.34,SYN-ACK
    192.168.1.2,10.0.0.2,45123,443,TCP,8,860,0.25,SYN-ACK
    192.168.1.1,10.0.0.3,53211,53,UDP,1,76,0.02,
    192.168.1.3,10.0.0.4,22,22,TCP,350,25420,124.5,ACK
    192.168.1.4,10.0.0.5,33,1433,TCP,15420,1245210,3600.5,ACK
    192.168.1.5,10.0.0.6,44424,80,TCP,6,780,0.15,SYN-ACK
    192.168.1.6,10.0.0.7,51231,443,TCP,9,950,0.28,SYN-ACK
    192.168.1.7,10.0.0.8,32451,53,UDP,1,82,0.03,
    """
    
    st.download_button(
        label="üì• T√©l√©charger un exemple de dataset",
        data=sample_data,
        file_name="sample_network_data.csv",
        mime="text/csv",
    )